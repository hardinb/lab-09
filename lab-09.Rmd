---
title: "Lab 9"
author: "Ben Hardin"
date: "2023-03-10"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r PACKAGES}
library(tidyverse) 
library(broom)
library(openintro)
```

### Exercise 1

Examining the distribution of course evaluation scores, we can see that there is a right skew, with the vast majority of people giving relatively high course evaluations. In other words, students generally almost never rate courses below the midpoint of the scale, and often rate courses at least 4 out of 5 or higher. I'm not too surprised by this, because I assume students would usually want to be some combination of kind and accurate in their evaluations (at least, in their quantitative ratings), and also because I assume most college instructors are usually quite competent at their job.

```{r score-dist}
ggplot(evals, aes(x = score))+
  geom_histogram()+
  scale_x_continuous(limits = c(1, 5))
```

### Exercise 2

The scatterplot below shows the relationship between course evals and average beauty ratings for each instructor.

```{r score-bty}
ggplot(evals, aes(x = score, y = bty_avg))+
  geom_point()
```

### Exercise 3

The same scatterplot is shown below, this time with the points "jittered". Jittering means that some slight random noise had been added to the data, which means that datapoints with the same value no longer overlap eachother in the graph. This allows us to see where there is a greater density of datapoints, in this case showing that there are quite a few more datapoints for high course evaluation scores and relatively few datapoints for more middling evaluation scores.

```{r jitter}
ggplot(evals, aes(x = score, y = bty_avg))+
  geom_jitter()
```

### Exercise 4

If we fit a linear regression with course evaluation scores predicted by average beauty ratings, we get the following linear model:

score = 3.88 + .067(average beauty)

```{r regression}
m_bty <- lm(data = evals, score ~ bty_avg)

summary(m_bty)
```

### Exercise 5

We can plot the slope of average beauty predicting course evaluation scores, which shows the slight positive linear relationship between course evals and beauty that we just got from our model.

```{r scatter-slope}
ggplot(evals, aes(x = score, y = bty_avg))+
  geom_point()+
  geom_smooth(method = "lm", se = F, color = "orange", linewidth = 2)+
  theme_classic()
```

### Exercise 6

A 1-point difference in a professor's average beauty rating is predicted, on average, to be associated with a .07-point difference in that professor's average course evaluation score in the same direction.

### Exercise 7

In this model, the intercept is 3.88, meaning that a professor with an average beauty rating of 0 is predicted to have a course evaluation score of 3.88. This intercept isn't really interpretable, because the beauty ratings were made on a scale of 1 - 10, so a rating of 0 is meaningless in this context.

### Exercise 8

The R^2 value of the model is .035, meaning about 3.5% of the variance in course evaluation scores is explained by average beauty ratings.

### Exercise 9

The linear model: score = 4.09 + .14(gender)

Slope of gender: Course evaluation ratings are predicted, on average, to be .14 points higher for male course instructors compared to female instructors.

Intercept: Female instructors are predicted, on average, to have a course evaluation score of about 4.09.

```{r cat-regression}
m_gen <- lm(data = evals, score ~ gender)

summary(m_gen)
```

### Exercise 10

Linear model for female instructors: score = 4.09

Linear model for male instructors: score = 4.23

### Exercise 11

Linear model: score = 4.28 + -.13(tenure track) + -.15(tenured)

Intercept: Teaching faculty are predicted, on average, to have a course evaluation score of about 4.28.

Slopes: Tenure-track faculty are predicted, on average, to have a course evaluation score of about 4.15. Tenured faculty are predicted, on average, to have a score of about 4.13.


``` {r rank}
m_rank <- lm(data = evals, score ~ rank)

summary(m_rank)
```

### Exercise 12

Let's create a new version of the rank variable, with tenure track as the base level.

```{r recode}
evals <- evals %>%
  mutate(rank_relevel = case_when(
    rank == "tenure track" ~ "0",
    rank == "tenured" ~ "tenured",
    rank == "teaching" ~ "teaching"))
```

### Exercise 13

Linear model: score = 4.15 + .13(teaching) + -.02(tenured)

Intercept: Tenure-track faculty are predicted, on average, to have a course evalutaion score of about 4.15

Slopes: Teaching faculty are predicted, on average, to have a course evaluation score of about 4.28. Tenured faculty are predicted, on average, to have a score of about 4.13.

The R^2 is .011, meaning that about 1.1% of the variance in evaluation scores is explained by tenure status.

```{r rank-redo}
m_rank_relevel <- lm(data = evals, score ~ rank_relevel)

summary(m_rank_relevel)
```

### Exercise 14

Now let's make a new variable that simply represents whether or not the instructor is eligible for tenure, regardless of whether they are currently tenured.

```{r tenurey}
evals <- evals %>%
  mutate(tenure_eligible = if_else(rank == "teaching", "no", "yes"))
```

### Exercise 15

Linear model: score = 4.28 + -.14(tenure eligible)

Intercept: Faculty who are not tenure eligible are predicted, on average, to have a course evaluation score of about 4.28.

Slope: Faculty who are tenure eligible are predicted, on average, to have a course evaluation score of about 4.14.

The R^2 is .011, meaning 1.1% of the variance in evaluation scores is explained by tenured status. 

It's also worth noting that the results and interpretations for Exercises 11, 13, and 15 are completely consistent with each other, which is good because they are all using the same data shaped in slightly different variations.

```{r tenure-regress}
m_tenure_eligible <- lm(data = evals, score ~ tenure_eligible)

summary(m_tenure_eligible)
```